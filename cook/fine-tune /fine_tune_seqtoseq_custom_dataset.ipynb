{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm8zsgXvlOGnapRL9zfWAY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/cloudflare-workers-langchain/blob/main/cook/fine-tune%20/fine_tune_seqtoseq_custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tune a Seq2Seq Model with a custom dataset\n",
        "Consists of both an encoder and a decoder (e.g., BART, T5). Suitable for tasks like translation, summarization, etc. Tokenizes both input and output sequences and uses DataCollatorForSeq2Seq for handling variable-length sequences.\n",
        "\n",
        "When fine-tuning a seq2seq model you'll need both 'text' (input sequence) and 'target' (output sequence) in your dataset. The dataset that was used in this script has a 'text' field and a 'keywords' field. Make sure you change the names appropriately.\n",
        "\n",
        "**This script uses an custom csv file as the dataset rather than importing one from Hugging Face.** Make sure you have gone through the data carefully before training your model.\n",
        "\n",
        "Make sure you set your runtime to T4 or better before running the script and look out for overfitting."
      ],
      "metadata": {
        "id": "0ek-Cn-B0aUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ2sT75ENJOJ"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install -U huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oQFZj17XNXTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# import file - make sure you set the correct path in your Google Drive (this would be the file I'm importing)\n",
        "# my file has two fields I'll be using called 'text' and 'keywords'\n",
        "file_path = '/content/drive/My Drive/8500_rows_keywords.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "aO4GieHDNZIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# remove any null values for the 'keywords' field - will cause issues later if there are any\n",
        "df = df[df['keywords'].notnull()]\n",
        "\n",
        "# Split dataset into training and temp (15% - 7.5% each) (for validation and testing)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.15, random_state=42)\n",
        "\n",
        "# Split temp into validation and testing\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "kwXtST0bNlRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "reWPlVweN3wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataset dict with the train, validate and test set\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "# print the dict\n",
        "dataset_dict"
      ],
      "metadata": {
        "id": "XudCrcgwN6-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) map out some examples from the dataset - my fields I'm using are text and keywords (make sure you set your own)\n",
        "def show_samples(dataset, num_samples=10, seed=42):\n",
        "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
        "    for example in sample:\n",
        "        print(f\"\\n'>> Text: {example['text']}'\")\n",
        "        print(f\"'>> Keywords: {example['keywords']}'\")\n",
        "\n",
        "\n",
        "show_samples(dataset_dict)"
      ],
      "metadata": {
        "id": "OzVLeVgvOJPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# set the correct model you'll be fine-tuning (see seq-to-seq model docs for more information)\n",
        "model_name = 'facebook/bart-large'\n",
        "# get the tokenizer for the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# check the token length of the keywords field - you can do this for both fields\n",
        "texts = dataset_dict['train']['keywords']\n",
        "\n",
        "# Tokenize all texts and find the maximum length (max for BART is 1024 tokens)\n",
        "max_token_length = max(len(tokenizer.encode(text, truncation=True)) for text in texts)\n",
        "print(f\"The longest text is {max_token_length} tokens long.\")"
      ],
      "metadata": {
        "id": "Q6SDIn5UOQxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert both the input text and the target text into a format suitable for training a sequence-to-sequence model\n",
        "# remember data preprocessing functions would look different if you were using a model with a different architecture, such as an encoder-only or decoder-only model.\n",
        "\n",
        "def get_feature(batch):\n",
        "  encodings = tokenizer(batch['text'], text_target=batch['keywords'],\n",
        "                        max_length=1024, truncation=True)\n",
        "\n",
        "  encodings = {'input_ids': encodings['input_ids'],\n",
        "               'attention_mask': encodings['attention_mask'],\n",
        "               'labels': encodings['labels']}\n",
        "\n",
        "  return encodings\n",
        "\n",
        "dataset_pt = dataset_dict.map(get_feature, batched=True)"
      ],
      "metadata": {
        "id": "bCx5ZNx4PFwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset should be formatted as PyTorch tensors with only the new fields\n",
        "# i.e. specifies which columns should be returned when accessing the data - only the new fields will be returned\n",
        "columns = ['input_ids', 'labels', 'attention_mask']\n",
        "dataset_pt.set_format(type='torch', columns=columns)\n",
        "\n",
        "dataset_pt"
      ],
      "metadata": {
        "id": "b_P8wsZSPigN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the data collator is responsible for dynamically padding the batches to the maximum length in each batch.\n",
        "# which is crucial for efficient training of transformer models like BART or T5.\n",
        "# padding will look different depending on the type of model you use, you can see here that this one is specifically for seq-to-seq\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "O1SHSKYzPnXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# start training the model\n",
        "# we're using the Trainer API which abstracts away a lot of complexity\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = 'bart_tech_keywords',\n",
        "    num_train_epochs=3, # your choice\n",
        "    warmup_steps = 500,\n",
        "    per_device_train_batch_size=4, # keep a small batch size when working with a small GPU\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay = 0.01, # helps prevent overfitting\n",
        "    logging_steps = 10,\n",
        "    evaluation_strategy = 'steps',\n",
        "    eval_steps=50, # base this on the size of your dataset and number of training epochs\n",
        "    save_steps=1e6,\n",
        "    gradient_accumulation_steps=16 # running this on a small GPU\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, data_collator=data_collator,\n",
        "                  train_dataset = dataset_pt['train'], eval_dataset = dataset_pt['validation'])\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# note: the trainer loss should go down, the validation loss may fluctuate for each evaluation step but consistently increasing validation, while training is going down could be a sign of overfitting"
      ],
      "metadata": {
        "id": "VpXNJ_YJPsv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "trainer.save_model('tech-keywords-extractor')"
      ],
      "metadata": {
        "id": "7zG4qM04QfQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# test the model using Hugging Face's pipeline\n",
        "pipe = pipeline('summarization', model='tech-keywords-extractor')\n",
        "\n",
        "# test the first item in the test set to see how it does\n",
        "test_text=dataset_dict['test'][0]['text']\n",
        "keywords = dataset_dict['test'][0]['keywords']\n",
        "print(\"the text: \", text_test)\n",
        "print(\"generated keywords: \", pipe(test_text))\n",
        "print(\"orginal keywords : \",keywords)"
      ],
      "metadata": {
        "id": "SsnP83-qQkOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over the test set to generate 50 examples at once\n",
        "for i in range(0, 50):\n",
        "    text_test = dataset_dict['test'][i]['text']\n",
        "    keywords = dataset_dict['test'][i]['keywords']\n",
        "    print(\"text: \", text_test)\n",
        "    print(\"generated keywords: \", pipe(text_test)[0]['summary_text'])\n",
        "    print(\"original keywords: \", keywords)"
      ],
      "metadata": {
        "id": "B1J_AtcwRFxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you're satisfied we can push it to Hugging Face\n",
        "# you'll need a token from your Hugging Face account to log in\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "mFj9ixfdRcDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you would replace your own name here\n",
        "# you do not need to create a repository beforehand\n",
        "trainer.push_to_hub(\"ilsilfverskiold/tech-keywords-extractor\")"
      ],
      "metadata": {
        "id": "2K29RCIoRvRw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}